# AWS Configuration
AWS_REGION=your-region
AWS_PROFILE=your-aws-profile

# Model Configuration
BEDROCK_MODEL_NAME=us.anthropic.claude-sonnet-4-5-20250929-v1:0

# Embedding Model Configuration (Titan v2)
TITAN_EMBED_MODEL=amazon.titan-embed-text-v2:0
TITAN_EMBED_DIMENSION=1024
TITAN_EMBED_NORMALIZE=true

# S3 Vectors Configuration
S3_VECTOR_BUCKET_NAME=survey-analysis-vectors
S3_VECTOR_INDEX_NAME=survey-responses

# S3 Vectors Performance Tuning
S3_VECTORS_BATCH_SIZE=500
S3_VECTORS_MAX_WORKERS=10
S3_VECTORS_RATE_LIMIT_RETRIES=5
S3_VECTORS_DETAILED_LOGS=false

# Data Configuration (provide your own CSV files)
SURVEY_CSV_PATH=data/your-survey-data.csv
EMBEDDING_MAX_ROWS=1000

# Logging
LOGFIRE_DIR=.cache/logfire

# ========================================
# AWS Batch Embedding Pipeline Configuration
# ========================================

# S3 bucket for CSV uploads (triggers pipeline)
CSV_UPLOAD_BUCKET=survey-csv-uploads-YOUR_ACCOUNT_ID

# S3 bucket for job checkpoints (fault tolerance)
CHECKPOINT_BUCKET=survey-embedding-checkpoints-YOUR_ACCOUNT_ID

# DynamoDB table for progress tracking
PROGRESS_TABLE=embedding-job-progress

# AWS Batch configuration
BATCH_JOB_QUEUE=survey-embedding-queue
BATCH_JOB_DEFINITION=survey-embedding-job

# ECR repository for batch container
ECR_REPOSITORY=YOUR_ACCOUNT_ID.dkr.ecr.YOUR_REGION.amazonaws.com/survey-embedding-job

# Batch job rate limiting (vectors/sec per container)
VECTORS_PER_SECOND=500

# Batch job configuration
EMBEDDING_BATCH_SIZE=100
UPLOAD_BATCH_SIZE=500
MAX_EMBEDDING_WORKERS=10
RATE_LIMIT_RETRIES=5

# Rows per chunk for array jobs (100K default)
ROWS_PER_CHUNK=100000
